---
layout: post
description: "None.NULL"
tweet-text: ""
author: taocp
tags:
- distributed
- stroage
categories:
- paper
---

# Bigtable:一个用于结构化数据的分布式表格存储系统

```
Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C. Hsieh, Deborah A. Wallach Mike Burrows, Tushar Chandra, Andrew Fikes, Robert E. Gruber 

		{fay,jeff,sanjay,wilsonh,kerr,m3b,tushar,fikes,gruber}@google.com
```

## Abstract
Bigtable是用于管理结构化数据的分布式表格存储系统，这些结构化数据被设计为能扩展(scale)到很大的数据量：跨千台商用机器(commodity servers)的PB级数据。Google的许多工程都将数据存储在Bigtable中，包括web indexing，Google Earth 和 Google Finance.这些应用对Bigtable提出了各种不同的需求，包括数据量（从url到网页内容，再到卫星图像）和延时（从后台批处理任务到实时数据服务）。尽管面临这样不同种类的需求，Bigtable还是成功地为这些Google产品地提供了一个灵活、高性能的解决方案。在这篇论文中，我们会描述Bigtable提供的简单的数据模型，这是客户端可以通过数据的layout和format来动态控制的，我们还会描述Bigtable的设计与实现。
<!--
(
scale:
commodity servers:
实时数据服务
)
-->

## 1 Introduction
过去的两年半里，我们设计、实现并部署了一套分布式表格存储系统，用于管理结构化数据，在Google它被称为Bigtable. Bigtable被设计为可以在PB级数据和千台机器上可靠扩展。Bigtable已经达到了一些目标：广泛的适用性、可扩展性(scalability)、高性能，还有高可用性(high availability)。Bigtable被超过60种Google产品和工程使用，包括Google Analytics, Google Finance, Orkut, Personalized Search, Writely, 和 Google Earth. 这些产品使用Bigtable来满足各种需求的负载，范围从面向吞吐的批处理任务到面向最终用户的延时敏感服务。Bigtable集群被这些产品用在很宽范围的配置下，从少数机器到上千台机器，存储达到上百TB数据。

在许多方面，Bigtable像一个数据库：它和数据库共享了很多实现策略。并行数据库和内存数据库达到了可扩展性和高性能，但是Bigtable提供了和这些系统不同的接口。Bigtable不支持全部的关系数据模型，作为代替，它给客户端提供了一个简单的数据模型，这个数据模型可以通过数据的layout和format来控制，并且允许客户端推断数据在底层存储系统中的位置属性。数据由行(row)和列(cloumn)名来索引，row和cloumn是任意字符串。Bigtable将数据视作未解释的字符串(uninterpreted strings)，尽管客户端经常将各种格式的结构化、半结构化数据序列化到这些字符串中。客户端可以通过仔细挑选schema来控制数据的位置。最后，Bigtable的schema参数使得客户端可以动态控制是从内存还是磁盘提供数据服务。
<!--
(
允许客户端推断数据在底层存储系统中的位置属性
Bigtable将数据视作未解释的字符串
Bigtable的schema参数使得客户端可以动态控制是从内存还是磁盘提供数据服务
)
-->

第二部分描述数据模型的更多细节，第三部分提供客户端API的overview，第四部分简单描述Bigtable依赖的底层Google基础设施。第五部分描述Bigtable实现的基本原理，第六部分描述一些我们用以提升Bigtable性能的改进(refinement)。第七部分提供Bigtable性能测试。第八部分描述一些例子，看看Google是怎样使用Bigtable的。第九部分讨论我们设计和支持Bigtable学习到的经验教训。最后，第十部分描述一些相关工作，第十一部分做出总结。

## 2 Data Model
Bigtable是一个稀疏的(sparse)、分布式、持久化的(persistent)多维排序map。这个map通过一个行键(row key)、列键(cloumn key)和一个时间戳(timestamp)来索引，每个map的值都是一个未解释的字符数组。

```
(row:string, column:string, time:int64) -> string
```

![图 1](http://taocp.github.io/img/2016-11-14-bigtable-cn/bigtable-f1.png)

```
图 1: 存储web page样例表的一小片。row名是反转的url."content"列族包含网页内容，"anchor"列族包含每个引用这个页面的anchor的文本。CNN的主页被Sports Illustrated和MY-look主页所引用，所以这一行包含了名为"anchor:cnnsi.com"和"anchor:my.look.ca"的两列。每个anchor单元格(cell)有一个版本，"contents"列有3个版本，分别对应时间戳t3，t5和t6.
```
<!--
(
anchor: HTML概念
)
-->

我们在考察了类似Bigtable系统各种各样潜在使用方法之后决定采用这种数据模型。举一个驱动我们做出一些设计决定的实际例子，假设我们想要保留一份网页内容和相关信息的集合的拷贝，这个拷贝能够用于许多不同的工程，让我们称这个特定的表格为webtable.在webtable中，我们可以用url作为row key，网页的各种属性作为column名，将网页内容存储在名为"contents:"的column中，对应其被抓取的时间戳，就像图1解释的那样。
<!--
(
sparse: 
persistent:
这个拷贝能够用于许多不同的工程
)
-->

### Rows
row key在表中是任意字符串（当前最大是64KB，尽管对绝大部分用户来说典型的大小是10-100字节）。对每一个单独的row key的读或写都是原子的（和这一行中被读或写的column数量无关），这样的一个设计决定使得客户端能够容易推断出系统在并发更新同一行时的行为。
<!--
(
单行原子性
例如A、B、C共3个客户端同时写同一行，不会出现最终结果是有几列是A写的，有几列是B写的，有几列是C写的，此时结果就完全乱了。因为原子性，所以最终的结果要么都是A，要么都是B，要么都是C写的；不考虑写失败的情形，最终的结果对应于ABC这3者中写请求最后执行的那个，覆盖了前2者的写结果
)
-->

Bigtable通过row key的字典序来维护数据。一个表格的row范围是动态分区的。每一个row范围被称为一个tablet，这是一个分布和负载均衡的单位。结果，读小范围的数据是高效的，通常只需要和少数机器进行通信。客户端能够通过选择row key来利用这个属性，使得它们的数据访问获取好的局部性。例如，在webtable中，同一个domain下的页面通过反转url中的主机名成分能够聚成一起形成连续的row。例如，我们存储"maps.google.com/index.html"的数据在key "com.google.maps/index.html"下。将同一个domain的网页存储在相邻的位置使得一些host和domain的分析更加高效。

### Column Families 列族
一些column key组成一个集合，被称为column family，这形成了基本的访问控制单元。所有存储在同一个column family下的数据通常具有相同的类型（我们将同一个column faimly下的数据一起压缩）。在数据被存储到一个column faimly 下的某个column之前，这个column faimly 必须已经被创建了，column faimly 被创建之后，其下的任意column key就可以被使用了。我们希望一个表中的column faimly 数量要少（最多几百个），并且column faimly 很少被更改。相反，一个表格可以包含不限数量的column.

一个column key以这样的语法命名：family:qualifier. column faimly 名必须是可打印的，但是qualifier可以是任意字符串。例如webtable表中"language"就是一个column faimly，它存储了一个web网页内容是何种语言写成的。我们在"language"这个column faimly 中只使用了一个column key，它存储每个web网页的语言标识符。对这个表来说，另一个有用的column faimly 是"anchor"，这个column faimly 中的每个column key都代表了一个单独的anchor，如图1所示。qualifier是发起引用网站的名字，这个cell的内容就是链接的文本。
<!--
(
qualifier
)
-->

访问控制、磁盘和内存的核算(accounting)都在column faimly 层面执行。在我们的webtable示例中，这些控制允许我们管理不同类型的应用程序：一些添加新的base data，一些读取base data并且创建派生(derived)的column faimly ，有些应用程序只被允许查看已经存在的数据（并且因为隐私原因，还可能只被允许查看已存在的column faimly 中的一部分）。

### Timestamps
Bigtable中的每个cell都包含同一个数据的多个版本；这些版本由timestamp来索引。Bigtable的timestamps是64-bit整数。它们由Bigtable赋值，这种情况下它们代表以微秒计数的真实时间(real time)，或者被客户端应用程序直接指定。应用程序必须自己生成唯一的timestamp来避免冲突。一个cell的不同版本以timestamp的降序排列，所以最新版本的数据会被最先读到。

![图 2](http://taocp.github.io/img/2016-11-14-bigtable-cn/bigtable-f2.png)

```
图 2: 向Bigtable写入
```

为了减少数据的版本管理负担，每个column family都支持两项设置，它们能告知Bigtable对cell内的版本自动执行垃圾收集。客户端能够指定一个cell保留最新的n个版本，或者只有足够新的版本被保留（例如，只保留最近7天写入的值）。

在我们webtable的示例中，我们将存储在"contents:"列中被抓取网页的timestamp设置为我们实际抓取这些页面的时间。上文描述的垃圾收集机制使得我们可以最多只保留每个页面最新的3个版本。

## 3 API
Bigtable的API提供创建、删除表格和column faimly 的功能，还支持改变集群、表格和column faimly元数据(metadata)，例如访问控制权限。

客户端应用程序可以写入或删除Bigtable中的值，根据单独的row来查询，或者在表中数据的一个子集上进行迭代(iterate)。图2展示了使用一个RowMutation抽象执行一些列更新的C++代码。（为了保证示例的简短无关细节已被省略。）对`Apply`的调用执行了一个对webtable的原子性修改：它添加一个anchor到www.cnn.com并且删除一个不同的anchor。

图3实现了使用Scanner抽象数据结构在某个特定行上迭代所有anchor的C++代码。客户端能够在多个column faimly 之间进行迭代，并且存在一些机制来限制scan得到的row,column和timestamp。例如，我们限制上述的扫描使得只生成那些column匹配正则表达式"anchor:*.cnn.com"或者timestamp在距今十天之内的anchor。

![图 3](http://taocp.github.io/img/2016-11-14-bigtable-cn/bigtable-f3.png)

```
图 3: 从Bigtable读取
```

Bigtable支持一些其它特性以允许用户通过更加丰富的方法来操作数据。首先，Bigtable支持单行事务，这使得针对存储在单个row key下数据的read-modify-write操作序列可以原子性的执行。Bigtable当前还不支持通用的跨行事务，尽管它向客户端提供了一个跨行批量写的接口。其次，Bigtable允许cell被用作一个整数计数器(integer counters)。最后，Bigtable支持在（Bigtable）服务器地址空间上执行客户端提供的脚本。这种脚本是用Sawzall写的，Sawzall是Google开发用于处理数据的语言。当前，我们的基于Sawzall的API还不允许客户端脚本写回到Bigtable中，但是允许各种形式的数据转换，基于任意任意表达式的过滤，以及通过各种操作实现summarization(summarization via a variety of operators).
<!--
(
read-modify-write:
integer counters:
summarization via a variety of operators:
)
-->

Bigtable能够被用在MapReduce中，后者是Google开发的运行大规模并行计算的框架。我们写了一个wrapper的集合以支持 将Bigtable作为输入和输出 的MapReduce任务。

## 4 Building Blocks
Bigtable构建在Google其它的一些基础设施之上。Bigtable使用分布式Google File System(GFS)来存储日志和数据文件。一个Bigtable集群通常在一个共享的机器池(a shared pool of machines)内运转，这个机器池运行着大量其它的分布式应用程序，Bigtable进程通常和其它应用程序共享同一台机器。Bigtable依赖一个集群管理系统(a cluster management system)来调度任务，管理共享机器的资源，处理机器故障，以及监控机器状态。
<!--
(
a cluster management system: brog
)
-->

Google SSTable文件格式被Bigtable内部用来存储数据。一个SSTable提供持久化、有序的、不可变更的(immutable)从key到value的映射，其中key和value都是任意的字符串。提供的操作有查询某个特定key关联的value，在指定的key的范围内迭代所有的key/value对。在内部，每个SSTable都包含了一个block的序列（通常每个block是64KB，但这是可配的）。一个block索引（存储在SSTable文件末尾）被用来定位block；索引在SStable被打开时载入内存。一次查询能够通过一次磁盘seek完成：首先对内存中的索引进行二分查找，找到合适的block，然后从磁盘读取适当的block。可选的，一个SSTable可以被完全映射到内存中，这使得我们执行查询和扫描时无须访问磁盘。

Bigtable依赖高可用和持久化的分布式锁服务Chubby。一个Chubby服务包含了五个活跃的replicas，其中一个被选做master，对请求做出积极响应。只要replicas中的大多数还活着并且可以互相通信那么服务就是可用的。Chubby使用paxos算法来保证replicas在面临故障时的一致性。Chubby提供了一个由目录和小文件组成的namespace。每个目录或者文件都可以被用作一个锁，并且对一个文件的读或写操作都是原子的。Chubby的客户端库提供了Chubby文件的一致性缓存。每个Chubby客户端和Chubby服务器维护一个session。一个客户端如果不能在session租约到期之前续租，那么本次session就会过期。当客户端的session过期后，它就会丢失每个锁以及打开的句柄(handle)。Chubby也能够在Chubby文件和目录上注册callback来获知其内容的变更或者session的过期。

Bigtable将Chubby用于各种任务上：确保在任何时刻最多只有一个活跃的master；存储Bigtable数据的自举位置(the bootstrap location of Bigtable data)（见5.1部分）；发现新加入或已经挂掉的tablet server（见5.2部分）；存储Bigtable的schema信息（每个table的column faimly 信息）；存储访问控制列表。如果Chubby长时间不可用，那么Bigtable也不再可用。我们最近在11个Chubby实例上的14个Bigtable集群上评估过这个影响。因Chubby不可用导致的Bigtable不可用情况，平均是0.0047%（包括Chubby的停服和网络故障），单个集群被Chubby影响导致不可用的是0.0326%。
<!--
(
We recently measured this effect in 14 Bigtable clusters spanning 11 Chubby instances. The average percentage of Bigtable server hours during which some data stored in Bigtable was not available due to Chubby unavailability (caused by either Chubby outages or network issues) was 0.0047%. The percentage for the single cluster that was most affected by Chubby unavailability was 0.0326%.
感觉挺难翻译的，不知道Bigtable的可用性是怎么定义的
)
-->

## 5 Implementation
Bigtable的实现主要包含三个组件：链接到每个客户端的库、一个master server，以及许多的tablet server. tablet server 能够动态的添加（或删除）到集群中以适应工作负载的变化。

master负责将tablet分配给tablet server，检测tablet server 的加入和过期，均衡tablet server 的负载，并且对GFS上的文件进行垃圾收集。另外，master还处理schema的变更，例如表格和column faimly 的创建。

每个tablet server 管理一个tablet组成的集合（通常每台tablet server有十到千个tablet）。tablet server 处理它已经load的tablet的读、写请求，并且对增长得过大的tablet执行split操作。

就像许多单master的分布式存储系统一样，客户端数据并不通过master：客户端直接和tablet server 通信进行读写。因为Bigtable的客户端不依赖master获取tablet的位置信息，所以绝大部分客户端不需要和master通信。因此，在实际情况下master的负载很轻。

一个Bigtable集群存储若干table。每个table由一个tablet组成的集合构成，每个tablet包含和一个row范围相关的所有数据。初始情况下，每个table只有一个tablet。随着table的增大，它会自动分裂成多个tablet，每个tablet的大小默认约为100-200MB.

### 5.1 Tablet location

我们使用3级层次类似于B+树来存储tablet的位置信息（图4）。

![图 4](http://taocp.github.io/img/2016-11-14-bigtable-cn/bigtable-f4.png)


```
图 4: tablet位置信息层次
```

第一层是一个存储在Chubby中的文件，其含有root tablet的位置。root tablet记录了专用的名为metadata表的位置，后者记录了所有tablet的位置。每个metadata表的tablet都含有用户tablet(user tablets)集合的位置信息。root tablet就是metadata表的第一个tablet，但是它会被特殊对待——它不会被分裂——确保tablet位置信息的层次结构不会超过3层。
<!--
(
 user tablets:相较于系统内部的tablet而言
)
-->

metadata表将tablet的位置存储在一个row key下，这个row key是由tablet所属table的标识符和最后一个row编码而成的。metadata表的每一行存储在内存中占用大约1KB。用一个适当的限制，metadata表tablet大小为128MB，那么我们的3级位置信息位置方案足够寻址2^34个tablet（或者128MB tablet的2^61字节）。

客户端库缓存tablet的位置。如果客户端不知道一个tablet的位置，或者它发现缓存的位置不正确，那么它会递归地在tablet位置信息层次体系中向上移动。如果客户端缓存为空，那么定位算法需要3轮网络通信，其中1次包括从Chubby读。如果客户端缓存已经失效，定位算法最多需要消耗6轮通信，因为过期的缓存条目只有在不命中时才会被发现（假设metadata移动不频繁）。尽管tablet的位置存储在内存中，所以不需要访问GFS，我们通过客户端库预取tablet位置进一步减少这种常见情形下的开销：每次读取metadata时都读走多个tablet的信息。

我们也在metadata中存储第二类信息，包括和每个tablet有关的所有事件日志（例如一个tablet server何时开始服务它）。这个信息对于调试和性能分析很有作用。

### 5.2 Tablet assignment

每个tablet在某个时刻只会被分配给一个tablet server 。master记录活着的tablet server 集合，当前已经分配给tablet server 的tablet分配状况，以及包括未分配的tablet。当一个tablet未分配并且一个tablet server 有足够的空间容纳它时，master就会通过向该tablet server 发送一个load请求来将这个tablet分配给这个tablet server.

Bigtable使用Chubby与tablet server保持联系。当一个tablet server 启动时，他会创建并获取一个排它锁，也就是Chubby特定目录下命名唯一的文件。master监控这个目录（server目录）以发现tablet server 。一个tablet server 停止服务它的tablet如果它丢失了排它锁：例如，因为网络分区导致tablet server 丢失了它和Chubby的session。（Chubby提供一个高效的机制来允许一个tablet server 检查它是否还持有自己的锁而without incurring network traffic。） 一个tablet server 会尝试获取它文件上的排它锁，只要文件还存在。如果文件不存在了，这个tablet server 就不再提供服务，所以它会主动退出。无论何时，一个tablet server 终止（例如，因为集群管理系统将tablet server 的机器从集群中移除），它都会尝试释放自己的锁，所以master会更快的将它的tablet重新分配。
<!--
(
without incurring network traffic:Chubby的啥特性
)
-->

master负责检测出一个tablet server 不再服务它的tablet，并且尽可能将这些tablet重新分配。为了检测某个tablet server 何时不再服务其tablet，master会周期性的询问每个tablet server 其锁的状态。如果某个tablet server 报告其丢失了自己的锁，或者master在经过了一些尝试之后不能和它取得联系，master就会尝试获取这个tablet server 文件的排它锁。如果master能够获取锁，那么Chubby是活着的并且tablet server 要么已经挂了要么不能联系到Chubby，所以master通过删除tablet server 的文件确保它不再服务。一旦tablet server 的文件被删除，master就能够将之前分配给它的tablet移动到未分配集合中。为了确保Bigtable集群不在master和Chubby间出现网络问题期间很脆弱，master会主动退出如果和Chubby的session过期。然而，如前所述，master的故障并不会改变给tablet server 分配tablet的状态。

当一个master被集群管理系统启动时，它需要发现当前tablet的分配情况才能改变它们。master在启动时执行下面的步骤：（1）master从Chubby获取唯一的master锁，这能防止master实例的并发执行。（2）master扫描Chubby中tablet server 目录以获知活着的tablet server 。（3）master和每个tablet server通信来发现哪些tablet已经分配了。（4）master扫描metadta表获知tablet的集合。当master扫描到未分配的tablet，它会将这个tablet加入到未分配集合，这将使得这个tablet能够被配分。

由此引发的一个问题就是在metadata表未被分配之前不能扫描它。因此，在启动扫描之前（第4步），master如果在第3步发现root tablet没有被分配则将它添加到未分配集合。这保证root tablet会被分配。因为root tablet包含了所有metadata tablet的名字，所以master在扫描完root tablet之后就知道了所有metadata tablet的信息。

已存在的tablet只有在以下情况改变：一个table被创建或者删除，两个tablet合并成一个更大的tablet，一个已经存在的tablet分裂成两个更小的tablet。master能够追踪到这些改变因为它发起所有操作除了分裂。tablet的分裂被特殊对待因为分裂是由tablet server发起的。tablet server 通过向metadata表记录新tablet的信息来提交(commit)分裂。当分裂被提交之后，它会通知master。万一split通知丢失了（或者因为tablet server或master挂掉），master会在它要求tablet server 执行load那个被分裂的tablet时发现新的tablet。tablet server 会通知master这个split，因为它发现metadata表中的tablet条目只是master命令它load的tablet一部分。
<!--
(
a分裂为b和c，master不知道a分裂的时候会命令tablet server去load a，但是分裂完成以后，tablet server发现b和c都只是a的一部分
)
-->

### 5.3 Tablet serving
tablet的状态持久化存储在GFS上，如图5所示。更新被提交到存储了redo记录的commit log里。对这些更新来说，最新提交的那部分存储在内存中一个排序的buffer中，称为memtable；更早的更新被存储在一系列SSTable中。为了恢复一个tablet，一个tablet server从metadata表读取它的元数据。这些meta数据包含了构成tablet的SSTable文件的列表，以及redo point的集合，这些redo point是可能含有tablet数据的commit log的指针。tablet server 读取SSTable的索引到内存中并且通过apply redo point之后的所有commit来重建memtable。


![图 5](http://taocp.github.io/img/2016-11-14-bigtable-cn/bigtable-f5.png)


```
图 5: tablet的表示
```

当一个写操作到达tablet server 时，tablet server 检查该请求是否格式正确(well-formed)并且请求者是否被授权执行修改。授权通过读取Chubby文件（几乎总是能在Chubby客户端缓存中被命中）中被许可writer列表实现。有效的修改会被写到commit log里。group commit用以提高大量小修改的吞吐。在写请求被提交之后，写的内容就会被插入到memtable中。

当一个读操作到达tablet server 时，类似地，tablet server 会检查该请求是否格式正确和合适的权限。一个有效的读操作在一系列SSTable和memtable合并的视图(merged view)上执行。因为SSTable和memtable是字典序的数据结构，所以合并的视图能被组织得很高效。
<!--
(
merged vew:
)
-->

到达的读、写请求在tablet的分裂、合并期间也能继续。

### 5.4 Compactions
随着写操作的进行，memtable的大小也在增加。当memtable的大小达到一个阈值时，这个memtable就会冻结，一个新的memtable被创建，被冻结的memtable会转换成一个SSTable并且写到GFS。这种被称为minor compaction的操作有两个目的：它减少了tablet server 的内存占用；减低了因为tablet server 挂掉而恢复期间要从commit log读取数据量。到达的读、写操作在compaction期间可以继续。

每个minor compaction创建一个新的SSTable。如果这种行为一直持续而没有检查(continued unchecked)，那么读操作可能需从任意数量SSTable来合并更新。取而代之，我们通过周期性地在后台执行merging compaction来限制这些文件的数量。一个merging compaction从一些SSTable和memtable读取数据，并且写到一个新的SSTable里。只要compaction完成，作为输入的SSTable和memtable就可以丢弃了。

将所有SSTable重写到一个SSTable的操作称为major compaction。非major compaction产生的SSTable可能包含特定的删除条目用以屏蔽(suppress)之前生成但此时还有效的SSTable中被删除的数据。另一方面，major compaction生成的SSTable不包含删除标记或者被删除的数据。Bigtable循环所有的tablet并定期地应用major compaction。这些major compaction允许Bigtable回收被删除数据占用的资源，还能确保被删除数据及时（in a timely fashion）从系统中消失，这对于存储敏感数据的服务言而非常重要。

## 6 refinements
上面描述的实现需要大量的改良以达到用户要求的高性能、高可用以及高可靠。这一部分详细描述部分实现细节以突出这些改进。

### Locality groups
客户端可以将多个column faimly 组合到一起，成为locality group。每个不同的SSTable是为每个tablet里每个locality group生成的。将通常不会被一起访问的column faimly 分离到单独的locality group 能够提高读效率。例如，webtable表中页面的meta数据（例如language和checksum）可以放在同一个locality group ，页面内容可以放在另一个不同的locality group ：一个想读取meta数据的应用程序完全不用读取页面内容。

另外，也能指定一些以单个locality group 为基础的有用参数，用以调优。例如，一个locality group 能够声明放在内存中。内存locality group 的SSTable会被延迟(lazily)加载到tablet server 的内存中。一旦加载，读取属于这些locality group 的column faimly就不再需要访问磁盘。这个特性对于小但访问频繁的数据非常有用：我们内部将它用于metadata表的location这个column faimly 。

### compression
客户端可以控制locality group 下的SSTable是否进行压缩，如果压缩，使用哪种压缩格式。用户指定的压缩格式被应用到每个SSTable的block（大小可以通过locality group 调优参数控制）上。尽管每个block单独压缩会损失一些空间，但可以从读取SSTable 的小部分不用解压整个文件中受益。许多客户端使用两遍定制的压缩方案(two-pass custom compression scheme)。第一遍使用Bentley and Mcllroy'方案，它在大的窗口上压缩公共字符串。第二遍使用一个快速压缩算法在小的16KB窗口上寻找重复数据。这两遍压缩都非常快——它们在现代机器上的压缩速率是100——200MB/s，解码速率是400——1000MB/s。

尽管在选择压缩算法时我们看中速度而不是空间的减少，这种两遍压缩方案在空间消耗上表现的也令人惊奇的好。例如，在webtable表中，我们使用这种压缩策略存储网页内容。一次实验中，我们在一个压缩的locality group 中存储了大量的文档。为了达到实验目的，我们限制每份文档只保留一个版本而不是全部可用版本。这种策略获得了10比1的空间压缩率。这比典型的Gzip在HTML上压缩率3比1到4比1要好太多了，原因是webtable行的布局：所有来自同一个主机(host)的网页存储在相邻的位置。这允许Bentley-Mcllroy算法识别来自相同主机的大量共享样板数据。许多应用程序，不仅仅是webtable，选择它们的row名以使相似的数据聚成簇，因而获得非常高的压缩率。压缩率在我们将同一个值的多个版本存入Bigtable时更高。

### Caching for read performance

为了提高读性能，tablet server 使用两级cache。Scan Cache是一个高层(higher-level)的cache，用于缓存SSTable接口返回的key-value对。Block Cache是低层(lower-level)的cache，用于缓存从GFS读取的SSTable文件的block。Scan Cache对于倾向于反复读取相同数据的应用程序非常有用。Block Cache对倾向于读取之前已读取数据附近数据的应用程序有用（例如，顺序读，或者一行很热数据的同一个locality group 里面的不同column）。

### bloom filters
就像5.3部分描述的，一次读操作必须从组成tablet的所有SSTable 读取。如果这些SSTable 不再内存中，我们最终需要执行多次磁盘访问。我们通过允许客户端指定特定locality group的SSTable创建bloom filter来减少访问磁盘的次数。bloom filter使得我们可以知道一个SSTable 是否含有一个特定的row/column对的数据。对特定的应用程序来说，tablet server 存储bloom filter 占用的少量内存可以显著减少读操作导致的磁盘seek次数。我们对bloom filter 的使用也意味着查询不存在的行或者列不需要访问磁盘。

### Commit-log implementation
如果我们为每个tablet都保存一个单独的commit log，那么就会对GFS上的大量文件进行并发写。依赖于底层文件系统GFS的服务器实现，这些写操作为了写到不同的物理log文件可能导致大量的磁盘seek。另外，每个tablet都有一个log文件也降低了group commit优化的效率，因为group会变得更小。为了解决这些问题，我们将每个tablet server 上的更新追加(append)到一个单独的commit log里，将不同tablet的更新混合到同一个物理log文件中。

使用一个log在普通操作时提供了重要的性能提升，但使得恢复变得更加复杂。当一个tablet server 挂掉，它服务的tablet会被移到大量的其它tablet server 上：每个tablet server 通常都会load少量的原来那个tablet server 的tablet。为了恢复tablet的状态，新tablet server 需要reapply原tablet server 写的commit log中对应tablet的更改。然而，这些tablet的更改混合到了同一个物理log文件中。一个方法是每个新的tablet server 都读取整个commit log文件并且只apply那些需要恢复tablet对应的条目。然而，这种方案下，如果给100个tablet server每个分配一个tablet，这个100个tablet从一个已挂掉的tablet server而来，那么log文件会被读取100次（每个tablet server 读取一次）。

我们通过先对commit log条目按<table, row name, log sequence number>进行排序避免重复读log。在已排序的输出中，所有对某个特定tablet的更改是连续的因此可以通过一次磁盘seek然后顺序读来高效完成。为了并行排序，我们将log文件划分成64MB大小的段，并在不同的tablet server 上对每个段进行并行的排序。排序进程由master负责协调，当一个tablet server 指明它需要从一些commit log中恢复更改时触发这次操作。

写commit log到GFS中有时会导致性能暂时较差(performance hiccups)，由于各种原因（例如，一个GFS机器遇到写崩溃(write crashes)，或者到达特定的三个GFS服务器的网络遇到了拥塞，或者负载过高）。为了保护更改不受GFS延迟抖动(latency spike)影响，每个tablet server实际上会有两个写log线程，每个都写自己的log文件，同一时刻只有一个激活(actively)。如果写激活log的性能较差，就会切换到另一个线程，commit log队列里的更改也由新的写log线程负责。log条目含有sequence number以使恢复过程可以消除因log切换导致的重复条目。

### Speeding up tablet recovery
如果master将一个tablet从一个tablet server 移动到另一个tablet server，原来的tablet server 首先对该tablet执行一次minor compaction。这次compaction减少commit log中uncompacted state而减少恢复时间。完成本次compaction之后，tablet server 停止服务这个tablet。在它实际unload这个tablet之前，tablet server 再执行一次（通常非常快）minor compaction以消除tablet server 在第一次minor compaction期间到达的uncompact state. 在第二次minor compaction完成之后，tablet就可以在另一台tablet server上被load而无须恢复任何log条目了。

### Exploiting immutability
除了SSTable cache，Bigtable系统的很多部分都可以简化因为所有SSTable 的一旦生成就不再改变这个事实。例如，我们在访问文件系统读SSTable 时不需要任何的同步操作。结果就是，跨行的并发控制可以实现得很高效。唯一会被读、写同时访问的可变数据结构是memtable，我们使得memtable的每一行都是写时复制(copy-on-write)的，允许读、写并行处理。
<!--
(
)
-->

因为SSTable 是不变的，永久性移除被删除数据的问题就转变成了废弃SSTable 的垃圾收集问题。每个tablet的SSTable 都在metadata表中注册。master在SSTable集合上执行mark-and-sweep垃圾回收删除废弃的SSTable，metadata表包含集合的root.(The master removes obsolete SSTables as a mark-and-sweep garbage collection over the set of SSTables, where the METADATA table contains the set of roots.)
<!--
(
)
-->

最后，SSTable的不变使得我们可以快速分裂tablet。不用为每个子tablet生成新的SSTable 集合，我们让子tablet共享父tablet的SSTable 文件。

## 7 Performance Evaluation

## 8 Real Applications

## 9 Lessons
在设计、实现、维护和支持Bigtable的过程中，我们收获了有用的经验，学习到一些有趣的教训。

一个教训是大的分布式系统会因为各种类型的故障而很脆弱，不仅仅是许多分布式协议中假设的标准的网络分区和停止运行故障。例如，我们因以下所有原因遇到过问题：内存和网络损坏，大的时钟漂移(large clock skew)，机器夯住，extended and asymmetric network partitions，我们使用的其它系统的bug（例如Chubby），超出GFS的配额，计划中和计划之外的硬件维护。随着我们对这些问题经验日益丰富，我们通过改变各种协议来解决这些问题。例如，我们给rpc机制添加了checksum。我们还通过删除系统一部分对另一部分做出的假设处理了一些问题。例如，我们不再假设给定的Chubby操作返回固定集合中的错误。

另一个教训是延迟添加新特性直到这个新特性被如何使用变得清晰是重要的。例如，我们最初计划添加支持通用目的事务的API。因为我们不急需使用它，所以没有立刻实现。现在我们有了很多真实的应用程序运行在Bigtable上，我们能够调查它们真实的需求，发现绝大部分应用程序只需要单行事务。在人们需要分布式事务的地方，最重要的运用是维护二级索引，我们计划添加特定的机制来满足这种需求。新的机制不如分布式事务那样通用，但是会更高效（特别是更新数百甚至更大范围的行）并且也会和我们乐观的跨数据中心复制方案更好地交互。

我们从支持Bigtable中学习到的一个实践教训就是合适的系统级监控（例如，监控Bigtable自身以及使用Bigtable的客户进程）很重要。例如，we extended our RPC system so that for a sample of the RPCs, it keeps a detailed trace of the important actions done on behalf of that RPC.这个特性允许我们检测和修正许多问题诸如tablet数据结构上锁的竞争，提交Bigtable变更到GFS的慢速写，当metadata表不可用时的访问卡住。另一个监控很有用的例子是每个Bigtable集群都会在Chubby中注册。这使得我们能够追踪到所有的集群，发现它们有多大，查看它们正在运行的软件版本，它们接收到的流量有多大，以及是否存在诸如未预料的大的延迟这样的问题。

我们学到的最重要经验是简单设计的价值。我们系统的大小（大约100,000行非测试代码）和实际上随着时间的流逝，代码以未预期的方式加入系统，我们发现代码和设计的清晰给维护和调式带来极大的帮助。一个例子就是tablet server 的成员协议。最开始的协议和简单：master周期性的给tablet server 发布租约，tablet server 在租约过期后自杀。不幸的是，这个协议在网络出现问题时严重降低了可用性，并且对master的恢复时间很敏感。我们数次重新设计协议直到协议执行良好。然而，最终的协议非常复杂并且依赖其它程序很少运用的Chubby特性。我们发现花费了非常多的时间在调试一些模糊的corner case，不仅仅是Bigtable的代码，还有Chubby。最终，我们废弃了这个协议并且迁移到一个更新更简单仅仅依赖Chubby广泛应用特性的协议上。

## 10 Related Work

## 11 Conclusions

我们已经描述了Bigtable，一个在Google用于存储结构化数据的分布式系统。Bigtable集群从2015.4起用于生产环境，在此之前我们花费了大约7人年在设计和实现上。至2006.8，超过六十个项目使用Bigtable. 我们的用户喜欢Bigtable提供的性能和高可用，并且在他们对资源的需求随着时间变化时可以简单的添加更多的机器来扩展他们的集群的容量。

给Bigtable不常见的接口，一个有趣的问题是我们的用户去适应使用会有多困难。新用户有时会不确定怎样使用接口是最佳的，尤其是他们习惯于使用支持通用目的事务的关系型数据库。然而，事实是许多的Google产品成功使用了Bigtable证明我们的设计在实践中工作良好。

我们正在实现Bigtable的一些新特性，例如支持二级索引，以及为构建多master副本跨数据中心复制的Bigtable的基础设施(infrastructure for building cross-data-center replicated Bigtables with multiple master replicas)。我们也开始部署Bigtable作为服务提供给产品组(product groups)，那么单独的组不需要维护它们自己的集群。随着我们服务集群的扩展，我们需要处理更多的Bigtable自身的资源共享问题。

最后，我们发现构建Google自己的存储解决方案非常重要。我们从为Bigtable设计自己的数据模型上获得了巨大的灵活性。另外，我们控制着Bigtable的实现，以及其它Bigtable依赖的基础设施，意味着如果它们出现瓶颈和低效的情况我们可以解决。

## Acknowledgements

## References
